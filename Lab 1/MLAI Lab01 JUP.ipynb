{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"This information is also in a pdf in this repository. It looks better there, sorry I ran out of time to figure this out. \nThe code is all in lab1.py.\n\n1.I would like to predict the price of gold by examining the annual prices of gold in the past. Logistic regression is the \nbest choice for this because I anticipate that the price will generally follow a linear pattern, which gradient descent \nalgorithms should be able to estimate.\n\n2. I chose a database that shows the annual prices of gold: https://www.kaggle.com/tunguz/gold-prices\nThe data was fairly simple so I did not do a lot of EDA to it. One thing I did do was simplify the dates in the csv, \nI just represented them as the numbers from 0-69 in my code instead of the full dates because this was easier to work with. \nThere were only two columns of data, so all of the data is used in my program with no other changes.\n\n3. The cost algorithm I used is known as the “Mean squared error“ which is represented by the equation:\n1/2M * M∑(i=1) ((θ0 + θ1 * x(i)) - y(i))2\n\nThere are two derivatives that are computed in my batch and stocashtic gradient descent functions using these equations:\n∂ / ∂ θ0 = 1 / M * M∑(i=1) ((θ0 + θ1 * x(i)) - y(i))\n∂ / ∂ θ1 = 1 / M * M∑(i=1) ((θ0 + θ1 * x(i)) - y(i)) * x(i)\n\n4.I used tensorflow to implement two more optimization algorithms, RMSprop and Adam. The RMSprop optimization performed better than Adam, but they both performed worse than my basic batch gradient descent.\n\nRMSprop usually had a loss somewhere in the one hundred thousands\nAdam usually had a loss somewhere in the two hundred thousands\nVanilla gradient descent was around seventy thousand.\n\nJudging from the data we should definitely not use optimization on this problem because they performed significantly worse than the default. My guess as to why would be because my data set, while generally having a linear rise over time, is fairly inconsistent and randomly makes large jumps messing with the algorithms.\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}
